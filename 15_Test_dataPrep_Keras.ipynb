{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15. Test_dataPrep_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFABFm0Go62EWdVRGoTGlG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anshul007/Anshul-Chaurasia/blob/master/15_Test_dataPrep_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMHjO-uyAnVH"
      },
      "source": [
        "## Spliting words with <i>text_to_word_sequence</i>\n",
        "\n",
        "Words are called token and the process of splitting token is called tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvH6tgHJ-1cU"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5jiKwF2AmhG",
        "outputId": "5cd996c0-4ae6-4d42-c313-3e89169ddf62"
      },
      "source": [
        "text = \"The quick brown fox jumped over the lazy dog\";\n",
        "\n",
        "#tokenize the doc\n",
        "results = text_to_word_sequence(text)\n",
        "print(results)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2WdnSauMSf6"
      },
      "source": [
        "## Note:\n",
        "by default <b>text_to_word_sequence</b> will do 3 things.\n",
        "\n",
        "1. Split word by space\n",
        "2. Filters out punchuation\n",
        "3. Convert text into lowercase (if lower = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd9dGkE_Nl7s"
      },
      "source": [
        "## Encoding with one hot\n",
        "Keras provide the one hot() that you can use to <b>tokenize</b> and <b>integer encode a text document</b> in one step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_at3rQsNlGv",
        "outputId": "d56ee7aa-463d-4997-a7c7-7d9d81b774b1"
      },
      "source": [
        "text = \"The quick brown fox jumped over the lazy dog\";\n",
        "\n",
        "#tokenize the doc\n",
        "results = text_to_word_sequence(text)\n",
        "\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPca1HgSWzq4"
      },
      "source": [
        "# import one hot from keras\n",
        "from keras.preprocessing.text import one_hot"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiwQjvN0XfoJ",
        "outputId": "798d8811-df4e-4487-d7f9-914096a50065"
      },
      "source": [
        "# printing the size of words\n",
        "print(\"Printing the word size from text: \"+ str(vocab_size))\n",
        "\n",
        "# integer encode the document\n",
        "result = one_hot(text, round(vocab_size*1.3))\n",
        "print(result)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing the word size from text: 8\n",
            "[9, 6, 9, 2, 9, 6, 9, 8, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj8A8SDMY7H1"
      },
      "source": [
        "## Hash Encoding with hashing trick\n",
        "\n",
        "It is simmilar to one_hot(), but it gives more flexibility. it also allow you to create your own Hash function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V1_HFV_YrWg"
      },
      "source": [
        "# importing hashing trick library\n",
        "from keras.preprocessing.text import hashing_trick"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7cB7UJyZZmT",
        "outputId": "791b1851-d291-4331-d2d1-db9f1f34be9d"
      },
      "source": [
        "# printing the size of words\n",
        "print(\"Printing the word size from text: \"+ str(vocab_size))\n",
        "\n",
        "# integer encode the documents\n",
        "result = hashing_trick(text, round(vocab_size*1.3), hash_function= \"md5\")\n",
        "print(result)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing the word size from text: 8\n",
            "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cPnC5uMli__"
      },
      "source": [
        "## Tokenizer API\n",
        "It can fit and reused to prepare multiple text documents.\n",
        "\n",
        "The tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxF_3dPcZ_jy"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9W90G91m5rM"
      },
      "source": [
        "# Defining doc\n",
        "doc = ['The quick',\n",
        " 'quick brown',\n",
        " 'brown fox',\n",
        " 'fox jumps',\n",
        " 'jumps the',\n",
        " 'the lazy',\n",
        " 'lazy dog']\n",
        "\n",
        " # create the tokenizer\n",
        "t = Tokenizer()\n",
        "\n",
        " # fit the tokenizer n the documents\n",
        "t.fit_on_texts(doc)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WhWlUNRnxFT"
      },
      "source": [
        "Once you fit, the tokenizer provides 4 attribute that you can use to query what has been learned about the doc\n",
        "\n",
        "1. <b>Word Count:</b> A dictonary of words and their count\n",
        "2. <b>Word Doc:</b> total # documents that were used to fit tokenizer\n",
        "3. <b>Word index:</b> A dictonary of words and their uniquly assign integer\n",
        "4. <b>Document Count:</b> A dictonary of words and how many documents ach appeared in.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5FpbQKenj0V",
        "outputId": "fe77fa64-75c6-4dfc-a1f0-c396e2b13c04"
      },
      "source": [
        "# i.e.\n",
        "\n",
        "print(t.word_counts)\n",
        "print(t.word_docs)\n",
        "print(t.word_index)\n",
        "print(t.document_count)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('the', 3), ('quick', 2), ('brown', 2), ('fox', 2), ('jumps', 2), ('lazy', 2), ('dog', 1)])\n",
            "defaultdict(<class 'int'>, {'quick': 2, 'the': 3, 'brown': 2, 'fox': 2, 'jumps': 2, 'lazy': 2, 'dog': 1})\n",
            "{'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'lazy': 6, 'dog': 7}\n",
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSmmh1YfqUlk",
        "outputId": "82517310-372b-4b79-c81c-5f3c113aeb91"
      },
      "source": [
        "# integer encode documents\n",
        "\n",
        "encode_doc = t.texts_to_matrix(doc, mode= \"count\")\n",
        "print(encode_doc)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgrvQ4iNrkNi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}